{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8d62cad",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe6344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "import plotly.express as px\n",
    "from scipy import stats\n",
    "import json\n",
    "import requests\n",
    "from matplotlib.path import Path as MplPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda4c49a",
   "metadata": {},
   "source": [
    "# Chatham House Trade data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb368476",
   "metadata": {},
   "source": [
    "## Importing and merging all trade data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7797b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and merging all trade data files\n",
    "\n",
    "paths = sorted(glob.glob(\"Input Data/Chatham House Data/Resource Trade Data *.xlsx\"))\n",
    "dfs = [pd.read_excel(p, sheet_name=\"Trades\") for p in paths]\n",
    "merged = pd.concat(dfs, ignore_index=True).drop_duplicates()\n",
    "merged = merged.sort_values([\"Year\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aebd46d",
   "metadata": {},
   "source": [
    "## Adjust for data quality and presence of sufficient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0165fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(merged.shape)\n",
    "# print(list(merged.columns))\n",
    "\n",
    "#Investigate missing values and drop columns where missing values exceed an arbitrary threshold\n",
    "merged.isna().mean()\n",
    "\n",
    "MISSING_THRESHOLD = 0.03\n",
    "\n",
    "missing_frac = merged.isna().mean()\n",
    "cols_to_keep = missing_frac[missing_frac <= MISSING_THRESHOLD].index\n",
    "\n",
    "reduced = merged[cols_to_keep].copy()\n",
    "# print(reduced.shape)\n",
    "\n",
    "print(f\"Dropped columns for not meeting the {100 - MISSING_THRESHOLD*100}% availability threshold:\", set(merged.columns) - set(reduced.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f337af9",
   "metadata": {},
   "source": [
    "## Prepare trade data for conversion to network - Aggregate trade values by year and country pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate trade values by year and country pairs (indexing columns) - currently the data has two rows per trade (one for each direction)\n",
    "# Also discard unnecessary columns\n",
    "\n",
    "INDEXING_COLS=[\"Year\", \"Importer\", \"Importer ISO3\", \"Exporter\", \"Exporter ISO3\"]\n",
    "VALUE_COLS=['Value (1000USD)', 'Weight (1000kg)']\n",
    "\n",
    "pairs_year = (\n",
    "    reduced\n",
    "    .groupby(INDEXING_COLS, as_index=False)[VALUE_COLS]\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "# Sort by Year and Total Value descending within each year\n",
    "\n",
    "pairs_year_sorted = pairs_year.sort_values(\n",
    "    [\"Year\", \"Value (1000USD)\"],\n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "pairs_year_sorted.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a75ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_year_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_year_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbff50ae",
   "metadata": {},
   "source": [
    "# Trade Concentration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hhi_normalized(series):\n",
    "    N = len(series)\n",
    "    if N <= 1:\n",
    "        return 1.0\n",
    "\n",
    "    shares = series / series.sum()\n",
    "    hhi_raw = (shares ** 2).sum()\n",
    "\n",
    "    # size-adjusted (normalized) HHI\n",
    "    return (hhi_raw - 1 / N) / (1 - 1 / N)\n",
    "\n",
    "# Calculate normalized HHI for each year based on trade values and weights\n",
    "\n",
    "df = pairs_year_sorted.copy()\n",
    "\n",
    "# ensure numeric\n",
    "for col in [\"Value (1000USD)\", \"Weight (1000kg)\"]:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "df = df.dropna(subset=[\"Year\", \"Value (1000USD)\", \"Weight (1000kg)\"])\n",
    "\n",
    "hhi_by_year = (\n",
    "    df\n",
    "    .groupby(\"Year\")\n",
    "    .agg(\n",
    "        HHI_value_norm=(\"Value (1000USD)\", hhi_normalized),\n",
    "        HHI_weight_norm=(\"Weight (1000kg)\", hhi_normalized),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.plot(\n",
    "    hhi_by_year[\"Year\"],\n",
    "    hhi_by_year[\"HHI_value_norm\"],\n",
    "    marker=\"o\",\n",
    "    label=\"Normalized HHI (Value, 1000 USD)\"\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    hhi_by_year[\"Year\"],\n",
    "    hhi_by_year[\"HHI_weight_norm\"],\n",
    "    marker=\"s\",\n",
    "    label=\"Normalized HHI (Weight, 1000 kg)\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Normalized Herfindahl‚ÄìHirschman Index\")\n",
    "plt.title(\"Global Soy Trade Concentration Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2b23d7",
   "metadata": {},
   "source": [
    "## Check for correlation between HHI by weight and by dollar value\n",
    "even though it is visually clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dcf68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if there's a high enough correlation between trade weight and trade value, then we can use one of them for further analysis\n",
    "\n",
    "rho, p = spearmanr(pairs_year_sorted[\"Weight (1000kg)\"], pairs_year_sorted[\"Value (1000USD)\"])\n",
    "\"{:.2f}\".format(float(rho)), \"{:.2f}\".format(float(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c1059",
   "metadata": {},
   "source": [
    "# Convert trade data to nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Graph from data\n",
    "\n",
    "df = pairs_year_sorted.copy()\n",
    "# Rename columns to short, consistent names\n",
    "df = df.rename(columns={\n",
    "    \"Exporter ISO3\": \"Exp_ISO3\",\n",
    "    \"Importer ISO3\": \"Imp_ISO3\",\n",
    "    \"Value (1000USD)\": \"Value\",\n",
    "    \"Weight (1000kg)\": \"Weight\",\n",
    "})\n",
    "\n",
    "# Ensure numeric columns are numeric\n",
    "df[\"Value\"] = pd.to_numeric(df[\"Value\"], errors=\"coerce\")\n",
    "# df[\"Weight\"] = pd.to_numeric(df[\"Weight\"], errors=\"coerce\")\n",
    "# using value since it correlates well with weight and is more commonly used in trade network analysis\n",
    "\n",
    "# Drop rows with missing essentials\n",
    "df = df.dropna(subset=[\"Year\", \"Exporter\", \"Exp_ISO3\", \"Imp_ISO3\", \"Importer\", \"Value\"])\n",
    "\n",
    "# Remove self-loops (country trading with itself)\n",
    "self_loops = df[df[\"Exp_ISO3\"] == df[\"Imp_ISO3\"]]\n",
    "df = df[df[\"Exp_ISO3\"] != df[\"Imp_ISO3\"]]\n",
    "print(f\"Removed {len(self_loops)} self-loops.\")\n",
    "\n",
    "# Build directed graphs for each year\n",
    "graphs_by_year = {}\n",
    "\n",
    "for year, dy in df.groupby(\"Year\"):\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for r in dy.itertuples(index=False):\n",
    "        if r.Value <= 0:\n",
    "            continue\n",
    "\n",
    "        # Add exporter node with attributes\n",
    "        G.add_node(\n",
    "            r.Exp_ISO3,\n",
    "            country=r.Exporter,\n",
    "            iso3=r.Exp_ISO3,\n",
    "        )\n",
    "\n",
    "        # Add importer node with attributes\n",
    "        G.add_node(\n",
    "            r.Imp_ISO3,\n",
    "            country=r.Importer,\n",
    "            iso3=r.Imp_ISO3,\n",
    "        )\n",
    "\n",
    "        # Add edge\n",
    "        G.add_edge(\n",
    "            r.Exp_ISO3,\n",
    "            r.Imp_ISO3,\n",
    "            weight=r.Value,\n",
    "            distance=1.0 / r.Value\n",
    "        )\n",
    "\n",
    "    graphs_by_year[year] = G\n",
    "\n",
    "print(f\"Constructed graphs for years: {list(graphs_by_year.keys())}\")\n",
    "# Print size of graphs for each year   \n",
    "for year, G in graphs_by_year.items():\n",
    "    print(f\"Year {year}: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcda651",
   "metadata": {},
   "source": [
    "## Calculate betweenness centrality for countries in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness_rows = []\n",
    "\n",
    "for year, G in graphs_by_year.items():\n",
    "\n",
    "    bc = nx.betweenness_centrality(\n",
    "        G,\n",
    "        weight=\"distance\",\n",
    "        normalized=True,\n",
    "    )\n",
    "\n",
    "    for iso3, score in bc.items():\n",
    "        betweenness_rows.append({\n",
    "            \"Year\": year,\n",
    "            \"ISO3\": iso3,\n",
    "            \"Country\": G.nodes[iso3].get(\"country\"),  # üëà pull name from node attrs\n",
    "            \"Betweenness\": score,\n",
    "        })\n",
    "\n",
    "betweenness_df = pd.DataFrame(betweenness_rows)\n",
    "\n",
    "print(\n",
    "    f\"Calculated betweenness centrality for {betweenness_df['ISO3'].nunique()} countries for {betweenness_df['Year'].nunique()} years.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da43fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bdb992",
   "metadata": {},
   "source": [
    "## Calculate slopes for between centrality (for trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f81dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope_of_series(years, values):\n",
    "    years = years.astype(float)\n",
    "    values = values.astype(float)\n",
    "    if len(values) < 2:\n",
    "        return np.nan\n",
    "    return np.polyfit(years, values, 1)[0]\n",
    "\n",
    "# ensure clean types\n",
    "bdf = betweenness_df.copy()\n",
    "bdf[\"Year\"] = pd.to_numeric(bdf[\"Year\"], errors=\"coerce\")\n",
    "bdf[\"Betweenness\"] = pd.to_numeric(bdf[\"Betweenness\"], errors=\"coerce\")\n",
    "bdf = bdf.dropna(subset=[\"Year\", \"ISO3\", \"Betweenness\"]).copy()\n",
    "bdf[\"Year\"] = bdf[\"Year\"].astype(int)\n",
    "\n",
    "betweenness_trend_df = (\n",
    "    bdf\n",
    "    .groupby([\"ISO3\", \"Country\"], dropna=False)\n",
    "    .apply(lambda d: pd.Series({\n",
    "        \"Slope\": slope_of_series(d[\"Year\"].values, d[\"Betweenness\"].values),\n",
    "        \"MeanBetweenness\": d[\"Betweenness\"].mean(),\n",
    "        \"MaxBetweenness\": d[\"Betweenness\"].max(),\n",
    "        \"FirstYear\": int(d[\"Year\"].min()),\n",
    "        \"LastYear\": int(d[\"Year\"].max()),\n",
    "        \"NumYears\": int(d[\"Year\"].nunique()),\n",
    "    }))\n",
    "    .reset_index()\n",
    "    .dropna(subset=[\"Slope\"])\n",
    ")\n",
    "\n",
    "print(f\"Trend rows: {len(betweenness_trend_df):,}\")\n",
    "betweenness_trend_df.sort_values(\"Slope\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f5dc3c",
   "metadata": {},
   "source": [
    "## Data outputs for graphing and use in report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5626a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 15 by Slope\n",
    "top15_slope = (\n",
    "    betweenness_trend_df.sort_values(\"Slope\", ascending=False)\n",
    "      .loc[:, [\"ISO3\", \"Country\", \"Slope\", \"MeanBetweenness\", \"MaxBetweenness\"]]\n",
    "      .head(15)\n",
    ")\n",
    "\n",
    "print(\"=== Top 15 Countries by Betweenness Slope ===\")\n",
    "display(top15_slope)\n",
    "\n",
    "# Top 15 by Mean Betweenness\n",
    "top15_mean_betweenness = (\n",
    "    betweenness_trend_df.sort_values(\"MeanBetweenness\", ascending=False)\n",
    "      .loc[:, [\"ISO3\", \"Country\", \"MeanBetweenness\", \"Slope\", \"MaxBetweenness\"]]\n",
    "      .head(15)\n",
    ")\n",
    "\n",
    "print(\"=== Top 15 Countries by Mean Betweenness ===\")\n",
    "display(top15_mean_betweenness)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d69e8",
   "metadata": {},
   "source": [
    "## Define map styles for chloropleth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d278179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a chloropleth map style\n",
    "\n",
    "MAP_STYLE = dict(\n",
    "    projection_type=\"natural earth\",\n",
    "    showcountries=True,\n",
    "    showcoastlines=True,\n",
    "    showland=True,\n",
    "    showocean=True,\n",
    "    lataxis_showgrid=False,\n",
    "    lonaxis_showgrid=False,\n",
    ")\n",
    "\n",
    "LAYOUT_STYLE = dict(\n",
    "    width=1400,\n",
    "    height=800,\n",
    "    margin=dict(l=20, r=20, t=60, b=20),\n",
    "    font=dict(size=14),\n",
    ")\n",
    "\n",
    "COLOR_SCALE = \"YlOrRd\"\n",
    "COLORBAR_TITLE = \"Betweenness slope\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe92318",
   "metadata": {},
   "source": [
    "## Use a technical definition for producing country\n",
    "Otherwise producing countries will also be visible in betweenness data, which is less interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88efbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify producing countries:\n",
    "# - export a lot (absolute scale)\n",
    "# - import almost nothing\n",
    "# - strong export >> import asymmetry\n",
    "\n",
    "MIN_EXPORT_SHARE = 0.03      # total share ‚â•3% of global exports\n",
    "MIN_NET_EXPORT_SHARE = 0.02  # net exports ‚â•2% of global exports\n",
    "MIN_RATIO = 5               # exports at least 5√ó imports\n",
    "\n",
    "df = pairs_year_sorted.copy()\n",
    "\n",
    "# Keep positive trade only\n",
    "df = df[\n",
    "    (df[\"Weight (1000kg)\"] > 0) &\n",
    "    df[\"Exporter ISO3\"].notna() &\n",
    "    df[\"Importer ISO3\"].notna()\n",
    "].copy()\n",
    "\n",
    "# Total exports and imports per country\n",
    "exports = df.groupby(\"Exporter ISO3\")[\"Weight (1000kg)\"].sum()\n",
    "imports = df.groupby(\"Importer ISO3\")[\"Weight (1000kg)\"].sum()\n",
    "\n",
    "balance = (\n",
    "    pd.concat([exports, imports], axis=1)\n",
    "    .fillna(0)\n",
    ")\n",
    "balance.columns = [\"exports\", \"imports\"]\n",
    "\n",
    "balance[\"net_exports\"] = balance[\"exports\"] - balance[\"imports\"]\n",
    "balance[\"export_import_ratio\"] = balance[\"exports\"] / balance[\"imports\"].replace(0, pd.NA)\n",
    "global_exports = balance[\"exports\"].sum()\n",
    "\n",
    "# countries that meet all criteria\n",
    "producing_df = balance[\n",
    "    (balance[\"exports\"] / global_exports >= MIN_EXPORT_SHARE) &\n",
    "    (balance[\"net_exports\"] / global_exports >= MIN_NET_EXPORT_SHARE) &\n",
    "    (balance[\"export_import_ratio\"] >= MIN_RATIO)\n",
    "].sort_values(\"exports\", ascending=False)\n",
    "\n",
    "PRODUCING_GEOS = set(producing_df.index)\n",
    "\n",
    "PRODUCING_GEOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829654f9",
   "metadata": {},
   "source": [
    "## Prepare data for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting: filter to top K rising intermediaries (by slope and mean betweenness), excluding producers\n",
    "FILTER_TOP_K = True\n",
    "TOP_K = 15\n",
    "\n",
    "betweenness_trend_topk = (\n",
    "    betweenness_trend_df\n",
    "    .loc[~betweenness_trend_df[\"ISO3\"].isin(PRODUCING_GEOS)] # exclude producers\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "if FILTER_TOP_K:\n",
    "    top_slope_iso3 = (\n",
    "        betweenness_trend_topk\n",
    "        .nlargest(TOP_K, \"Slope\")[\"ISO3\"]\n",
    "    )\n",
    "\n",
    "    top_mean_iso3 = (\n",
    "        betweenness_trend_topk\n",
    "        .nlargest(TOP_K, \"MeanBetweenness\")[\"ISO3\"]\n",
    "    )\n",
    "\n",
    "    betweenness_trend_topk[\"Slope_plot\"] = (\n",
    "        betweenness_trend_topk[\"Slope\"]\n",
    "        .where(betweenness_trend_topk[\"ISO3\"].isin(top_slope_iso3))\n",
    "    )\n",
    "\n",
    "    betweenness_trend_topk[\"MeanBetweenness_plot\"] = (\n",
    "        betweenness_trend_topk[\"MeanBetweenness\"]\n",
    "        .where(betweenness_trend_topk[\"ISO3\"].isin(top_mean_iso3))\n",
    "    )\n",
    "else:\n",
    "    betweenness_trend_topk[\"Slope_plot\"] = betweenness_trend_topk[\"Slope\"]\n",
    "    betweenness_trend_topk[\"MeanBetweenness_plot\"] = betweenness_trend_topk[\"MeanBetweenness\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7009374",
   "metadata": {},
   "source": [
    "## Generate chloropleth maps for high centrality countries and those with ascending centralities (top K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c69f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = px.choropleth(\n",
    "    betweenness_trend_topk,\n",
    "    locations=\"ISO3\",\n",
    "    locationmode=\"ISO-3\",\n",
    "    color=\"MeanBetweenness_plot\",\n",
    "    hover_name=\"Country\",\n",
    "    hover_data={\n",
    "        \"ISO3\": True,\n",
    "        \"Slope\": \":.3e\",\n",
    "        \"MeanBetweenness\": \":.3e\",\n",
    "        \"MaxBetweenness\": \":.3e\",\n",
    "        \"NumYears\": True,\n",
    "        \"FirstYear\": True,\n",
    "        \"LastYear\": True,\n",
    "    },\n",
    "    color_continuous_scale=COLOR_SCALE,\n",
    "    title=f\"Top {TOP_K} Rising Intermediaries by Mean Betweenness (Producers Omitted)\" if FILTER_TOP_K else \"Betweenness Trend (Mean Betweenness Over Time, Producers Omitted)\"\n",
    ")\n",
    "\n",
    "fig1.update_geos(**MAP_STYLE)\n",
    "fig1.update_layout(**LAYOUT_STYLE)\n",
    "fig1.update_layout(coloraxis_colorbar_title=COLORBAR_TITLE)\n",
    "\n",
    "fig1.show()\n",
    "\n",
    "fig2 = px.choropleth(\n",
    "    betweenness_trend_topk,\n",
    "    locations=\"ISO3\",\n",
    "    locationmode=\"ISO-3\",\n",
    "    color=\"Slope_plot\",\n",
    "    hover_name=\"Country\",\n",
    "    hover_data={\n",
    "        \"ISO3\": True,\n",
    "        \"Slope\": \":.3e\",\n",
    "        \"MeanBetweenness\": \":.3e\",\n",
    "        \"MaxBetweenness\": \":.3e\",\n",
    "        \"NumYears\": True,\n",
    "        \"FirstYear\": True,\n",
    "        \"LastYear\": True,\n",
    "    },\n",
    "    color_continuous_scale=COLOR_SCALE,\n",
    "    title=f\"Top {TOP_K} Rising Intermediaries by Betweenness Slope (Producers Omitted)\" if FILTER_TOP_K else \"Betweenness Trend (Slope Over Time, Producers Omitted)\"\n",
    ")\n",
    "\n",
    "fig2.update_geos(**MAP_STYLE)\n",
    "fig2.update_layout(**LAYOUT_STYLE)\n",
    "fig2.update_layout(coloraxis_colorbar_title=COLORBAR_TITLE)\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e19ae8e",
   "metadata": {},
   "source": [
    "## Generate a combined version for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f2026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "df = betweenness_trend_topk.copy()\n",
    "\n",
    "# Top-15 intermediaries are exactly those with a real Slope_plot value\n",
    "top15 = df[\"Slope_plot\"].notna()\n",
    "\n",
    "# Positive slope among top-15\n",
    "pos_top15 = df.loc[top15 & (df[\"Slope_plot\"] > 0)].copy()\n",
    "\n",
    "# Base map with choropleth coloring by mean betweenness (only for top-K, if filtering)\n",
    "fig = px.choropleth(\n",
    "    df,\n",
    "    locations=\"ISO3\",\n",
    "    locationmode=\"ISO-3\",\n",
    "    color=\"MeanBetweenness_plot\",\n",
    "    hover_name=\"Country\",\n",
    "    hover_data={\n",
    "        \"ISO3\": True,\n",
    "        \"Slope\": \":.3e\",\n",
    "        \"MeanBetweenness\": \":.3e\",\n",
    "        \"MaxBetweenness\": \":.3e\",\n",
    "        \"NumYears\": True,\n",
    "        \"FirstYear\": True,\n",
    "        \"LastYear\": True,\n",
    "    },\n",
    "    color_continuous_scale=COLOR_SCALE,\n",
    "    title=f\"Top {TOP_K}: Mean betweenness (fill) + ‚ñ≤ if Slope_plot > 0\" if FILTER_TOP_K\n",
    "          else \"Mean betweenness (fill) + ‚ñ≤ if Slope_plot > 0\"\n",
    ")\n",
    "\n",
    "fig.update_geos(**MAP_STYLE)\n",
    "fig.update_layout(**LAYOUT_STYLE)\n",
    "fig.update_layout(coloraxis_colorbar_title=\"Mean betweenness\")\n",
    "\n",
    "# Overlay positive slope (‚ñ≤) markers for top-15 intermediaries\n",
    "fig.add_trace(\n",
    "    go.Scattergeo(\n",
    "        locations=pos_top15[\"ISO3\"],\n",
    "        locationmode=\"ISO-3\",\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            symbol=\"triangle-up\",\n",
    "            size=10,\n",
    "            line=dict(width=0.6),\n",
    "            opacity=0.95\n",
    "        ),\n",
    "        name=\"Positive slope (Top-15 only)\",\n",
    "        text=pos_top15[\"Country\"],\n",
    "        hovertemplate=\"<b>%{text}</b><br>ISO3=%{location}<br>Slope_plot=%{customdata:.3e}<extra></extra>\",\n",
    "        customdata=pos_top15[\"Slope_plot\"],\n",
    "        showlegend=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"betweenness_combined.png\", scale=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae152f1",
   "metadata": {},
   "source": [
    "## Experimental: see if import routes into the Netherlands changed over the last period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLD = \"NLD\"\n",
    "TOP_K = 10  # top suppliers to track\n",
    "\n",
    "df = pairs_year_sorted.copy()\n",
    "value_col = None\n",
    "for c in [\"Value (1000USD)\", \"Total Value (1000USD)\", \"value\", \"Value\"]:\n",
    "    if c in df.columns:\n",
    "        value_col = c\n",
    "        break\n",
    "if value_col is None:\n",
    "    raise ValueError(\"Couldn't find a trade value column.\")\n",
    "df[value_col] = pd.to_numeric(df[value_col], errors=\"coerce\")\n",
    "\n",
    "nld_imports = df[\n",
    "    (df[\"Importer ISO3\"] == NLD) &\n",
    "    (df[\"Exporter ISO3\"] != NLD)\n",
    "].dropna(subset=[\"Year\", \"Exporter ISO3\", value_col])\n",
    "\n",
    "top_sets = {}\n",
    "for year, g in nld_imports.groupby(\"Year\"):\n",
    "    suppliers = (\n",
    "        g.groupby(\"Exporter ISO3\")[value_col].sum()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(TOP_K)\n",
    "        .index\n",
    "        .tolist()\n",
    "    )\n",
    "    top_sets[year] = set(suppliers)\n",
    "\n",
    "years = sorted(top_sets.keys())\n",
    "rows = []\n",
    "for i in range(1, len(years)):\n",
    "    y0, y1 = years[i-1], years[i]\n",
    "    A, B = top_sets[y0], top_sets[y1]\n",
    "    jaccard = len(A & B) / len(A | B) if (A | B) else np.nan\n",
    "    entered = len(B - A)\n",
    "    exited = len(A - B)\n",
    "    rows.append({\"Year\": y1, \"JaccardTopK\": jaccard, \"EnteredTopK\": entered, \"ExitedTopK\": exited})\n",
    "\n",
    "churn = pd.DataFrame(rows)\n",
    "print(churn)\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(churn[\"Year\"], churn[\"JaccardTopK\"], marker=\"o\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(f\"Jaccard similarity of top-{TOP_K} suppliers\")\n",
    "plt.title(f\"Netherlands: stability / lock-in of top-{TOP_K} import suppliers\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(churn[\"Year\"], churn[\"EnteredTopK\"], marker=\"o\", label=\"Entered\")\n",
    "plt.plot(churn[\"Year\"], churn[\"ExitedTopK\"], marker=\"o\", label=\"Exited\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(f\"Count of suppliers in top-{TOP_K}\")\n",
    "plt.title(f\"Netherlands: churn in top-{TOP_K} suppliers\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb92a9b",
   "metadata": {},
   "source": [
    "## Experimental: Very basic simulation of removal of largest input intermediary, to see which country would substitute.\n",
    "\n",
    "Not interesting because NL imports directly from Brazil, which is the dominant producer. Mostly trade would be routed through USA, with India and China in two years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3279115",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NLD = \"NLD\"\n",
    "\n",
    "# --- import values to identify top supplier per year ---\n",
    "df = pairs_year_sorted.copy()\n",
    "value_col = None\n",
    "for c in [\"Value (1000USD)\", \"Total Value (1000USD)\", \"value\", \"Value\"]:\n",
    "    if c in df.columns:\n",
    "        value_col = c\n",
    "        break\n",
    "if value_col is None:\n",
    "    raise ValueError(\"Couldn't find a trade value column.\")\n",
    "df[value_col] = pd.to_numeric(df[value_col], errors=\"coerce\")\n",
    "\n",
    "nld_imports = df[\n",
    "    (df[\"Importer ISO3\"] == NLD) &\n",
    "    (df[\"Exporter ISO3\"] != NLD)\n",
    "].dropna(subset=[\"Year\", \"Exporter ISO3\", value_col])\n",
    "\n",
    "imports_by_year = (\n",
    "    nld_imports\n",
    "    .groupby([\"Year\", \"Exporter ISO3\"])[value_col].sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "stress_rows = []\n",
    "\n",
    "for year, G in graphs_by_year.items():\n",
    "    if NLD not in G:\n",
    "        continue\n",
    "\n",
    "    # NLD betweenness + reachability baseline\n",
    "    bc = nx.betweenness_centrality(G, weight=\"distance\", normalized=True)\n",
    "    nld_bc = bc.get(NLD, np.nan)\n",
    "    base_reach = len(nx.ancestors(G, NLD))  # nodes that can reach NLD\n",
    "\n",
    "    # top supplier to NLD that year (by direct import value)\n",
    "    dy = imports_by_year[imports_by_year[\"Year\"] == year]\n",
    "    if dy.empty:\n",
    "        continue\n",
    "\n",
    "    total_import = dy[value_col].sum()\n",
    "    top_supplier = dy.sort_values(value_col, ascending=False).iloc[0][\"Exporter ISO3\"]\n",
    "    top_share = dy.sort_values(value_col, ascending=False).iloc[0][value_col] / total_import\n",
    "\n",
    "    # top intermediary in the whole graph (highest betweenness excluding NLD itself)\n",
    "    top_intermediary = max(\n",
    "        (n for n in bc.keys() if n != NLD),\n",
    "        key=lambda n: bc[n],\n",
    "        default=None\n",
    "    )\n",
    "\n",
    "    # test removal of top supplier\n",
    "    GA = G.copy()\n",
    "    if top_supplier in GA:\n",
    "        GA.remove_node(top_supplier)\n",
    "    bcA = nx.betweenness_centrality(GA, weight=\"distance\", normalized=True) if NLD in GA else {}\n",
    "    reachA = len(nx.ancestors(GA, NLD)) if NLD in GA else 0\n",
    "    nld_bc_A = bcA.get(NLD, np.nan)\n",
    "\n",
    "    # test removal of top intermediary\n",
    "    GB = G.copy()\n",
    "    if top_intermediary in GB:\n",
    "        GB.remove_node(top_intermediary)\n",
    "    bcB = nx.betweenness_centrality(GB, weight=\"distance\", normalized=True) if NLD in GB else {}\n",
    "    reachB = len(nx.ancestors(GB, NLD)) if NLD in GB else 0\n",
    "    nld_bc_B = bcB.get(NLD, np.nan)\n",
    "\n",
    "    stress_rows.append({\n",
    "        \"Year\": year,\n",
    "        \"TopSupplier\": top_supplier,\n",
    "        \"TopSupplierShareOfNLDImports\": top_share,\n",
    "        \"BaseReachToNLD\": base_reach,\n",
    "        \"ReachAfterRemoveTopSupplier\": reachA,\n",
    "        \"ReachAfterRemoveTopIntermediary\": reachB,\n",
    "        \"NLD_Betweenness\": nld_bc,\n",
    "        \"NLD_Betweenness_AfterRemoveTopSupplier\": nld_bc_A,\n",
    "        \"NLD_Betweenness_AfterRemoveTopIntermediary\": nld_bc_B,\n",
    "        \"TopIntermediary\": top_intermediary,\n",
    "        \"TopIntermediary_Betweenness\": bc.get(top_intermediary, np.nan)\n",
    "    })\n",
    "\n",
    "stress_df = pd.DataFrame(stress_rows).sort_values(\"Year\")\n",
    "display(stress_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b850ee",
   "metadata": {},
   "source": [
    "## Importing Trase Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Trase Data\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"Input Data/Trase Import Data\")\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in DATA_DIR.glob(\"*.xlsx\"):\n",
    "    # read year from the \"Filters\" sheet, cell C3 on\n",
    "    filters = pd.read_excel(\n",
    "        file,\n",
    "        sheet_name=\"Filters\",\n",
    "        header=None\n",
    "    )\n",
    "    year = int(filters.iloc[2, 2])  # C3 ‚Üí row 2, col 2 (0-based)\n",
    "\n",
    "    # read main data from \"Data\" sheet\n",
    "    df = pd.read_excel(file, sheet_name=\"Data\")\n",
    "\n",
    "    df[\"Year\"] = year\n",
    "    df[\"SourceFile\"] = file.name  # optional but useful\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "df_trase = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df_trase.drop(columns=[\"SourceFile\",\"country_of_first_import\"], inplace=True)\n",
    "\n",
    "df_trase.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc9eba5",
   "metadata": {},
   "source": [
    "## Importing Mapbiomas data on deforestation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a275068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing MapBiomas Deforestation Data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path = \"Input Data/Brazil Deforestation Biome State.xlsx\"\n",
    "deforestation_raw = pd.read_excel(path,sheet_name=\"DEFORESTATION\")\n",
    "\n",
    "id_cols = [\n",
    "    \"country\", \"biome\", \"state\", \"class\", \"transition_name\",\n",
    "    \"class_level_0\", \"class_level_1\", \"class_level_2\",\n",
    "    \"class_level_3\", \"class_level_4\"\n",
    "]\n",
    "\n",
    "year_cols = [c for c in deforestation_raw.columns if c.isdigit()]\n",
    "\n",
    "deforestation_long = (\n",
    "    deforestation_raw\n",
    "    .melt(\n",
    "        id_vars=id_cols,\n",
    "        value_vars=year_cols,\n",
    "        var_name=\"year\",\n",
    "        value_name=\"deforested_area\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# optional: make year numeric\n",
    "deforestation_long[\"year\"] = deforestation_long[\"year\"].astype(int)\n",
    "\n",
    "deforestation_long[(deforestation_long[\"year\"] == 2020) & (deforestation_long[\"state\"] == \"Mato Grosso\")]\n",
    "\n",
    "deforestation_aggregated = (\n",
    "    deforestation_long\n",
    "    .groupby(\n",
    "        [\"country\", \"state\", \"year\"],\n",
    "        as_index=False\n",
    "    )[\"deforested_area\"]\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "deforestation_aggregated.drop(columns=[\"country\"], inplace=True)\n",
    "\n",
    "deforestation_aggregated\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7228172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Deforestation and Trase Data to get a soy panel dataset\n",
    "\n",
    "defor = deforestation_aggregated.copy()\n",
    "\n",
    "defor[\"state\"] = defor[\"state\"].str.upper().str.strip()\n",
    "defor[\"year\"] = defor[\"year\"].astype(int)\n",
    "\n",
    "# Optional: rename for clarity\n",
    "defor = defor.rename(columns={\"deforested_area\": \"deforestation_ha\"})\n",
    "\n",
    "\n",
    "soy = df_trase.copy()\n",
    "\n",
    "soy = soy.query(\"state != 'UNKNOWN STATE'\")\n",
    "soy[\"state\"] = soy[\"state\"].str.upper().str.strip()\n",
    "soy[\"Year\"] = soy[\"Year\"].astype(int)\n",
    "\n",
    "soy = (\n",
    "    soy\n",
    "    .groupby([\"state\", \"Year\"], as_index=False)\n",
    "    .agg(soy_volume=(\"volume\", \"sum\"))\n",
    ")\n",
    "\n",
    "state_map = {\n",
    "    \"ACRE\": \"ACRE\",\n",
    "    \"ALAGOAS\": \"ALAGOAS\",\n",
    "    \"AMAPA\": \"AMAP√Å\",\n",
    "    \"AMAZONAS\": \"AMAZONAS\",\n",
    "    \"BAHIA\": \"BAHIA\",\n",
    "    \"CEARA\": \"CEAR√Å\",\n",
    "    \"DISTRITO FEDERAL\": \"DISTRITO FEDERAL\",\n",
    "    \"ESPIRITO SANTO\": \"ESP√çRITO SANTO\",\n",
    "    \"GOIAS\": \"GOI√ÅS\",\n",
    "    \"MARANHAO\": \"MARANH√ÉO\",\n",
    "    \"MATO GROSSO\": \"MATO GROSSO\",\n",
    "    \"MATO GROSSO DO SUL\": \"MATO GROSSO DO SUL\",\n",
    "    \"MINAS GERAIS\": \"MINAS GERAIS\",\n",
    "    \"PARA\": \"PAR√Å\",\n",
    "    \"PARAIBA\": \"PARA√çBA\",\n",
    "    \"PARANA\": \"PARAN√Å\",\n",
    "    \"PERNAMBUCO\": \"PERNAMBUCO\",\n",
    "    \"PIAUI\": \"PIAU√ç\",\n",
    "    \"RONDONIA\": \"ROND√îNIA\",\n",
    "    \"RORAIMA\": \"RORAIMA\",\n",
    "    \"SANTA CATARINA\": \"SANTA CATARINA\",\n",
    "    \"SAO PAULO\": \"S√ÉO PAULO\",\n",
    "    \"SERGIPE\": \"SERGIPE\",\n",
    "    \"TOCANTINS\": \"TOCANTINS\",\n",
    "}\n",
    "\n",
    "soy[\"state_mb\"] = soy[\"state\"].map(state_map)\n",
    "defor[\"state_mb\"] = defor[\"state\"].str.upper()\n",
    "soy = soy.dropna(subset=[\"state_mb\"])\n",
    "\n",
    "soy_panel = soy.merge(\n",
    "    defor,\n",
    "    left_on=[\"state_mb\", \"Year\"],\n",
    "    right_on=[\"state_mb\", \"year\"],\n",
    "    how=\"inner\"\n",
    ").drop(columns=[\"state_x\", \"state_y\", \"year\"])\n",
    "\n",
    "soy_panel.sample(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6264c9",
   "metadata": {},
   "source": [
    "## Define basic State data for Brazil for mergin and further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed0a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes for states for merging\n",
    "state_to_uf = {\n",
    "    \"ACRE\": \"AC\", \"ALAGOAS\": \"AL\", \"AMAP√Å\": \"AP\", \"AMAZONAS\": \"AM\",\n",
    "    \"BAHIA\": \"BA\", \"CEAR√Å\": \"CE\", \"DISTRITO FEDERAL\": \"DF\",\n",
    "    \"ESP√çRITO SANTO\": \"ES\", \"GOI√ÅS\": \"GO\", \"MARANH√ÉO\": \"MA\",\n",
    "    \"MATO GROSSO\": \"MT\", \"MATO GROSSO DO SUL\": \"MS\",\n",
    "    \"MINAS GERAIS\": \"MG\", \"PAR√Å\": \"PA\", \"PARAN√Å\": \"PR\",\n",
    "    \"PIAU√ç\": \"PI\", \"ROND√îNIA\": \"RO\", \"RORAIMA\": \"RR\",\n",
    "    \"SANTA CATARINA\": \"SC\", \"S√ÉO PAULO\": \"SP\", \"TOCANTINS\": \"TO\",\n",
    "    \"PARA√çBA\": \"PB\", \"PERNAMBUCO\": \"PE\", \"RIO DE JANEIRO\": \"RJ\",\n",
    "    \"RIO GRANDE DO NORTE\": \"RN\", \"RIO GRANDE DO SUL\": \"RS\", \"SERGIPE\": \"SE\",\n",
    "}\n",
    "\n",
    "# ensure all states are represented\n",
    "all_states = pd.DataFrame({\n",
    "    \"uf\": [\n",
    "        \"AC\",\"AL\",\"AP\",\"AM\",\"BA\",\"CE\",\"DF\",\"ES\",\"GO\",\"MA\",\"MT\",\"MS\",\n",
    "        \"MG\",\"PA\",\"PB\",\"PR\",\"PE\",\"PI\",\"RJ\",\"RN\",\"RS\",\"RO\",\"RR\",\"SC\",\n",
    "        \"SP\",\"SE\",\"TO\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# replace with official file from IBGE?\n",
    "brazil_states = requests.get(\n",
    "    \"https://raw.githubusercontent.com/codeforamerica/click_that_hood/master/public/data/brazil-states.geojson\"\n",
    ").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb4ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate imports\n",
    "\n",
    "state_imports = (\n",
    "    soy_panel\n",
    "    .groupby(\"state_mb\", as_index=False)\n",
    "    .agg(mean_soy=(\"soy_volume\", \"mean\"))   # or \"sum\"\n",
    ")\n",
    "\n",
    "df = state_imports.copy()\n",
    "df[\"uf\"] = df[\"state_mb\"].map(state_to_uf)\n",
    "\n",
    "plot_df = all_states.merge(df[[\"uf\", \"mean_soy\"]], on=\"uf\", how=\"left\")\n",
    "\n",
    "# Merge areas\n",
    "\n",
    "areas = pd.read_csv(\"Input Data/brasil_states_area.csv\")\n",
    "areas[\"uf\"] = areas[\"uf\"].str.upper()\n",
    "\n",
    "plot_df = plot_df.merge(areas, on=\"uf\", how=\"left\")\n",
    "\n",
    "# Clculate per area\n",
    "\n",
    "plot_df[\"import_density\"] = (\n",
    "    plot_df[\"mean_soy\"] / plot_df[\"area_km2\"]   # imports per km¬≤\n",
    ")\n",
    "\n",
    "# log scale for more clarity (otherwise the orders of magnitude difference would throw it off)\n",
    "plot_df[\"log_density\"] = np.log10(plot_df[\"import_density\"].clip(lower=1e-12))\n",
    "\n",
    "\n",
    "vmin = plot_df[\"log_density\"].min(skipna=True)\n",
    "vmax = plot_df[\"log_density\"].max(skipna=True)\n",
    "\n",
    "sentinel = vmin - 1.0\n",
    "plot_df[\"fill\"] = plot_df[\"log_density\"].fillna(sentinel) #create a sentinel value in case values are missing. Can't be 0.\n",
    "\n",
    "eps = 1e-6\n",
    "grey = \"#e6e6e6\"\n",
    "reds = px.colors.sequential.Reds\n",
    "\n",
    "colorscale = [[0.0, grey], [eps, grey]]\n",
    "for i, c in enumerate(reds):\n",
    "    t = eps + (1 - eps) * (i / (len(reds) - 1))\n",
    "    colorscale.append([t, c])\n",
    "\n",
    "# Chloropleth\n",
    "fig = px.choropleth(\n",
    "    plot_df,\n",
    "    geojson=brazil_states,\n",
    "    locations=\"uf\",\n",
    "    featureidkey=\"properties.sigla\",\n",
    "    color=\"fill\",\n",
    "    color_continuous_scale=colorscale,\n",
    "    range_color=(sentinel, vmax),\n",
    "    hover_data={\n",
    "        \"uf\": True,\n",
    "        \"mean_soy\": \":,.0f\",\n",
    "        \"area_km2\": \":,.0f\",\n",
    "        \"import_density\": \":.3f\",\n",
    "        \"log_density\": False,\n",
    "        \"fill\": False,\n",
    "    },\n",
    "    title=\"NL Soy Import Intensity by Brazilian State (imports per km¬≤)\",\n",
    ")\n",
    "\n",
    "fig.update_geos(\n",
    "    visible=False,\n",
    "    scope=\"south america\",\n",
    "    projection_type=\"mercator\",\n",
    "    lonaxis_range=[-75, -30],\n",
    "    lataxis_range=[-35, 6],\n",
    ")\n",
    "\n",
    "fig.update_traces(marker_line_color=\"grey\", marker_line_width=0.6)\n",
    "\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=50, b=0),\n",
    "    coloraxis_colorbar=dict(title=\"log‚ÇÅ‚ÇÄ imports / km¬≤\"),\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"brazil_soy_import_density.png\", scale=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e5ed12",
   "metadata": {},
   "source": [
    "## Soy imports to NL vs deforestation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb2f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Change lag to test other hypothesis - no clear theory\n",
    "LAG_YRS = 0 \n",
    "\n",
    "df = soy_panel.sort_values([\"state_mb\", \"Year\"]).copy()\n",
    "\n",
    "# lag within state\n",
    "df[\"soy_lag\"] = df.groupby(\"state_mb\")[\"soy_volume\"].shift(LAG_YRS)\n",
    "\n",
    "# drop missing\n",
    "df = df.dropna(subset=[\"soy_lag\", \"deforestation_ha\"])\n",
    "\n",
    "# optional but recommended: log-log\n",
    "x = np.log1p(df[\"soy_lag\"].to_numpy())\n",
    "y = np.log1p(df[\"deforestation_ha\"].to_numpy())\n",
    "\n",
    "# Regression\n",
    "X = np.column_stack([np.ones_like(x), x])         # [1, x]\n",
    "beta, *_ = np.linalg.lstsq(X, y, rcond=None)      # [intercept, slope]\n",
    "intercept, slope = beta\n",
    "\n",
    "y_hat = intercept + slope * x\n",
    "ss_res = np.sum((y - y_hat) ** 2)\n",
    "ss_tot = np.sum((y - y.mean()) ** 2)\n",
    "r2 = 1 - ss_res / ss_tot\n",
    "\n",
    "p_value = None\n",
    "\n",
    "n = len(x)\n",
    "se2 = ss_res / (n - 2)\n",
    "sxx = np.sum((x - x.mean()) ** 2)\n",
    "se_slope = np.sqrt(se2 / sxx)\n",
    "t = slope / se_slope\n",
    "p_value = 2 * (1 - stats.t.cdf(abs(t), df=n - 2))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(x, y, alpha=0.6)\n",
    "\n",
    "x_line = np.linspace(x.min(), x.max(), 100)\n",
    "plt.plot(x_line, intercept + slope * x_line)\n",
    "\n",
    "plt.xlabel(f\"log(soy imports + 1), lag = {LAG_YRS}\")\n",
    "plt.ylabel(\"log(deforestation + 1)\")\n",
    "plt.title(f\"Soy imports vs deforestation (lag {LAG_YRS} years)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(f\"n = {len(x)}\")\n",
    "print(f\"log(deforestation+1) = {intercept:.4f} + {slope:.4f} * log(soy+1)\")\n",
    "print(f\"R¬≤ = {r2:.4f}\")\n",
    "if p_value is not None:\n",
    "    print(f\"p-value (slope) = {p_value:.3g}\")\n",
    "else:\n",
    "    print(\"p-value (slope) = (scipy not available / not compatible)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fdf850",
   "metadata": {},
   "source": [
    "## Experimental: Calculate soy import (to NL) exposure by state of production\n",
    "Not used for report because of incomplete import data, and dominance of import source over deforestation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480fdb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_exposure = (\n",
    "    soy_panel\n",
    "    .groupby(\"state_mb\", as_index=False)\n",
    "    .agg(\n",
    "        mean_soy=(\"soy_volume\", \"mean\"),\n",
    "        mean_defor=(\"deforestation_ha\", \"mean\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "state_exposure[\"exposure_score\"] = state_exposure[\"mean_soy\"] * state_exposure[\"mean_defor\"]\n",
    "\n",
    "df = state_exposure.copy()\n",
    "df[\"uf\"] = df[\"state_mb\"].map(state_to_uf)\n",
    "\n",
    "plot_df = all_states.merge(df, on=\"uf\", how=\"left\")\n",
    "\n",
    "areas[\"uf\"] = areas[\"uf\"].str.upper()\n",
    "\n",
    "plot_df = plot_df.merge(areas, on=\"uf\", how=\"left\").drop(columns=[\"state_mb\"])\n",
    "\n",
    "# Expecting areas has area_km2; convert to hectares\n",
    "plot_df[\"area_ha\"] = plot_df[\"area_km2\"] * 100.0\n",
    "\n",
    "# Intensity: deforestation per unit state area (ha / ha)\n",
    "plot_df[\"defor_intensity\"] = plot_df[\"mean_defor\"] / plot_df[\"area_ha\"]\n",
    "\n",
    "# Exposure adjusted by intensity (soy √ó deforestation intensity)\n",
    "plot_df[\"exposure_intensity\"] = plot_df[\"mean_soy\"] * plot_df[\"defor_intensity\"]\n",
    "\n",
    "# Now log-scale the adjusted exposure\n",
    "plot_df[\"log_exposure\"] = np.log10(plot_df[\"exposure_intensity\"].clip(lower=1e-12))\n",
    "\n",
    "# Decompose exposure into additive log components\n",
    "plot_df[\"log_imports\"] = np.log10(plot_df[\"mean_soy\"].clip(lower=1e-12))\n",
    "plot_df[\"log_defor_intensity\"] = np.log10(plot_df[\"defor_intensity\"].clip(lower=1e-12))\n",
    "\n",
    "# check for math errors\n",
    "plot_df[\"log_exposure_check\"] = (\n",
    "    plot_df[\"log_imports\"] + plot_df[\"log_defor_intensity\"]\n",
    ")\n",
    "\n",
    "vmin = plot_df[\"log_exposure\"].min(skipna=True)\n",
    "vmax = plot_df[\"log_exposure\"].max(skipna=True)\n",
    "\n",
    "if pd.isna(vmin) or pd.isna(vmax):\n",
    "    vmin, vmax = 0.0, 1.0\n",
    "\n",
    "sentinel = vmin - 1.0\n",
    "plot_df[\"fill\"] = plot_df[\"log_exposure\"].fillna(sentinel)\n",
    "\n",
    "eps = 1e-6\n",
    "grey = \"#e6e6e6\"\n",
    "reds = px.colors.sequential.Reds\n",
    "\n",
    "colorscale = [\n",
    "    [0.0, grey],\n",
    "    [eps, grey],\n",
    "]\n",
    "for i, c in enumerate(reds):\n",
    "    t = eps + (1 - eps) * (i / (len(reds) - 1))\n",
    "    colorscale.append([t, c])\n",
    "\n",
    "fig = px.choropleth(\n",
    "    plot_df,\n",
    "    geojson=brazil_states,\n",
    "    locations=\"uf\",\n",
    "    featureidkey=\"properties.sigla\",\n",
    "    color=\"fill\",\n",
    "    color_continuous_scale=colorscale,\n",
    "    range_color=(sentinel, vmax),\n",
    "    hover_name=\"state_name\",\n",
    "    hover_data={\n",
    "        \"mean_soy\": \":,.0f\",\n",
    "        \"mean_defor\": \":,.0f\",\n",
    "        \"defor_intensity\": \":.3e\",\n",
    "        \"exposure_intensity\": \":.3e\",\n",
    "        \"uf\": True,\n",
    "        \"log_exposure\": False,\n",
    "        \"fill\": False,\n",
    "    },\n",
    "    title=\"NL Soy Import Exposure to Deforestation by Brazilian State (Area-normalised)\",\n",
    ")\n",
    "\n",
    "fig.update_geos(\n",
    "    visible=False,\n",
    "    scope=\"south america\",\n",
    "    projection_type=\"mercator\",\n",
    "    lonaxis_range=[-75, -30],\n",
    "    lataxis_range=[-35, 6],\n",
    ")\n",
    "\n",
    "fig.update_traces(marker_line_color=\"black\", marker_line_width=0.6)\n",
    "\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=50, b=0),\n",
    "    coloraxis_colorbar=dict(title=\"log‚ÇÅ‚ÇÄ exposure\"),\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"brazil_soy_exposure.png\", scale=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54950cf1",
   "metadata": {},
   "source": [
    "# Climate Soy Yields changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56734382",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIMATE_PATH = \"Input Data/Soy Yield Brazil 3.5 vs 1.5 Annual.csv\"\n",
    "\n",
    "df = pd.read_csv(CLIMATE_PATH, skiprows=8)\n",
    "\n",
    "print(\"Raw preview:\")\n",
    "display(df.head())\n",
    "\n",
    "# first column = latitude\n",
    "df = df.rename(columns={df.columns[0]: \"lat\"})\n",
    "\n",
    "# longitude values come from column names\n",
    "lon_vals = np.array([float(c) for c in df.columns[1:]])\n",
    "lat_vals = pd.to_numeric(df[\"lat\"]).to_numpy()\n",
    "\n",
    "# grid values\n",
    "Z = df.iloc[:, 1:].astype(float).to_numpy()\n",
    "\n",
    "print(\"Lat long grid shape\", Z.shape)\n",
    "print (\"Lat long range:\")\n",
    "print(\"Lon range:\", lon_vals.min(), \"‚Üí\", lon_vals.max())\n",
    "print(\"Lat range:\", lat_vals.min(), \"‚Üí\", lat_vals.max())\n",
    "print(\"Range of pp changes (for validation):\", np.nanmin(Z), \"‚Üí\", np.nanmax(Z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed6e3e",
   "metadata": {},
   "source": [
    "## Plotting of states by climate change impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7591f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reusing plot_df from previous exposure calculation\n",
    "exposure_col = \"log_exposure\" if \"log_exposure\" in plot_df.columns else \"fill\"\n",
    "\n",
    "# Map bounds wirh margin\n",
    "LON_RANGE = (-75, -30)\n",
    "LAT_RANGE = (-35, 6)\n",
    "\n",
    "\n",
    "clim = pd.read_csv(CLIMATE_PATH, skiprows=8)\n",
    "clim = clim.rename(columns={clim.columns[0]: \"lat\"})\n",
    "\n",
    "lon = np.array([float(c) for c in clim.columns[1:]], dtype=float)\n",
    "lat = pd.to_numeric(clim[\"lat\"], errors=\"coerce\").to_numpy(dtype=float)\n",
    "Z = clim.iloc[:, 1:].apply(pd.to_numeric, errors=\"coerce\").to_numpy(dtype=float)\n",
    "\n",
    "# sort by latitude for nicer display\n",
    "order = np.argsort(lat)\n",
    "lat = lat[order]\n",
    "Z = Z[order, :]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        x=lon,\n",
    "        y=lat,\n",
    "        z=Z,\n",
    "        zsmooth=\"best\",\n",
    "\n",
    "        colorscale=[\n",
    "            [0.0,  \"#d73027\"],   \n",
    "            [0.5,  \"#ffffff\"],\n",
    "            [1.0,  \"#1a9850\"], \n",
    "        ],\n",
    "\n",
    "        zmid=0,  # ‚Üê makes white exactly zero-centered\n",
    "\n",
    "        hovertemplate=\"lon=%{x}<br>lat=%{y}<br>climate=%{z:.3f}<extra></extra>\",\n",
    "        colorbar=dict(title=\"Climate impact\"),\n",
    "        name=\"Climate grid\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "brazil_states = requests.get(\n",
    "    \"https://raw.githubusercontent.com/codeforamerica/click_that_hood/master/public/data/brazil-states.geojson\"\n",
    ").json()\n",
    "\n",
    "def add_polygon_lines(coords, name=None):\n",
    "    \"\"\"\n",
    "    coords: either Polygon ring list or MultiPolygon list of polygons\n",
    "    Adds outlines as go.Scatter line traces (cartesian).\n",
    "    \"\"\"\n",
    "    if not coords:\n",
    "        return\n",
    "\n",
    "    is_multipolygon = isinstance(coords[0][0][0], (list, tuple))\n",
    "\n",
    "    polygons = coords if is_multipolygon else [coords]\n",
    "\n",
    "    for poly in polygons:\n",
    "        # outer ring is poly[0]\n",
    "        ring = poly[0]\n",
    "        xs = [p[0] for p in ring]\n",
    "        ys = [p[1] for p in ring]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=xs,\n",
    "                y=ys,\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=1, color=\"rgba(120,120,120,0.7)\"),\n",
    "                name=name if name else \"State border\",\n",
    "                showlegend=False,\n",
    "                hoverinfo=\"skip\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "# draw all states\n",
    "for feat in brazil_states[\"features\"]:\n",
    "    geom = feat[\"geometry\"]\n",
    "    name = feat[\"properties\"].get(\"sigla\", None)\n",
    "    add_polygon_lines(geom[\"coordinates\"], name=name)\n",
    "\n",
    "\n",
    "# Plot like a map\n",
    "fig.update_xaxes(\n",
    "    title=\"Longitude\",\n",
    "    range=list(LON_RANGE),\n",
    "    showgrid=False, # remove gridlines\n",
    "    zeroline=False\n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    title=\"Latitude\",\n",
    "    range=list(LAT_RANGE),\n",
    "    scaleanchor=\"x\",\n",
    "    scaleratio=1,\n",
    "    showgrid=False,\n",
    "    zeroline=False\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Percentage point impact of climate change on soy yield in Brazil with 3.5¬∞C vs 1.5¬∞C warming\",\n",
    "    margin=dict(l=10, r=10, t=50, b=10),\n",
    "    height=800,\n",
    "    plot_bgcolor=\"white\",\n",
    "    paper_bgcolor=\"white\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"brazil_soy_yields_climate_impact.png\", scale=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea59aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build mesh of cell centers\n",
    "LON, LAT = np.meshgrid(lon, lat)\n",
    "points = np.column_stack([LON.ravel(), LAT.ravel()])\n",
    "\n",
    "rows = []\n",
    "\n",
    "for feat in brazil_states[\"features\"]:\n",
    "    uf = feat[\"properties\"][\"sigla\"]\n",
    "    geom = feat[\"geometry\"][\"coordinates\"]\n",
    "\n",
    "    if feat[\"geometry\"][\"type\"] == \"Polygon\":\n",
    "        rings = [geom[0]]\n",
    "    else:\n",
    "        rings = [poly[0] for poly in geom]\n",
    "\n",
    "    mask = np.zeros(len(points), dtype=bool)\n",
    "\n",
    "    for ring in rings:\n",
    "        path = MplPath(ring)\n",
    "        mask |= path.contains_points(points)\n",
    "\n",
    "    state_vals = Z.ravel()[mask]\n",
    "\n",
    "    mean_change_pp = np.nan if len(state_vals) == 0 else np.nanmean(state_vals)\n",
    "\n",
    "    rows.append({\n",
    "        \"uf\": uf,\n",
    "        \"mean_climate_change_pp\": mean_change_pp\n",
    "    })\n",
    "\n",
    "climate_state = pd.DataFrame(rows)\n",
    "\n",
    "result = plot_df.merge(climate_state, on=\"uf\", how=\"left\")\n",
    "\n",
    "# drop states with missing data\n",
    "result = result.dropna(subset=[\"mean_soy\", \"mean_climate_change_pp\"]).copy()\n",
    "\n",
    "result[\"mean_climate_change\"] = result[\"mean_climate_change_pp\"] / 100.0\n",
    "\n",
    "result[\"adjusted_imports\"] = (\n",
    "    result[\"mean_soy\"] * (1 + result[\"mean_climate_change\"])\n",
    ")\n",
    "\n",
    "result[\"import_change\"] = (\n",
    "    result[\"adjusted_imports\"] - result[\"mean_soy\"]\n",
    ")\n",
    "\n",
    "display(\n",
    "    result[[\n",
    "        \"uf\",\n",
    "        \"mean_soy\",\n",
    "        \"mean_climate_change_pp\",\n",
    "        \"mean_climate_change\",\n",
    "        \"adjusted_imports\",\n",
    "        \"import_change\"\n",
    "    ]].sort_values(\"mean_soy\", ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee13376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_total = result[\"mean_soy\"].sum()\n",
    "adjusted_total = result[\"adjusted_imports\"].sum()\n",
    "\n",
    "shock_abs = adjusted_total - baseline_total\n",
    "shock_rel = shock_abs / baseline_total\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"baseline_imports\": [baseline_total],\n",
    "    \"adjusted_imports\": [adjusted_total],\n",
    "    \"absolute_shock\": [shock_abs],\n",
    "    \"relative_shock_frac\": [shock_rel],\n",
    "    \"relative_shock_percent\": [shock_rel * 100],\n",
    "})\n",
    "\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eab4f3",
   "metadata": {},
   "source": [
    "## Experimental: Simulate change in import because of production/yield adjustments under the climate change scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67627df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# input DF should have: uf, mean_soy, mean_climate_change_pp, adjusted_imports, import_change\n",
    "df = result.copy()\n",
    "\n",
    "plot_df = all_states.merge(df, on=\"uf\", how=\"left\")\n",
    "\n",
    "# plotting projected import change\n",
    "value_col = \"import_change\" \n",
    "\n",
    "vmin = plot_df[value_col].min(skipna=True)\n",
    "vmax = plot_df[value_col].max(skipna=True)\n",
    "\n",
    "if pd.isna(vmin) or pd.isna(vmax):\n",
    "    vmin, vmax = -1.0, 1.0\n",
    "\n",
    "absmax = max(abs(vmin), abs(vmax))\n",
    "vmin, vmax = -absmax, absmax\n",
    "\n",
    "# sentinel for missing data -> grey\n",
    "sentinel = vmin - absmax * 0.05 - 1.0\n",
    "plot_df[\"fill\"] = plot_df[value_col].fillna(sentinel)\n",
    "\n",
    "# colors definition: grey for missing, then red-white-green diverging\n",
    "eps = 1e-6\n",
    "grey = \"#e6e6e6\"\n",
    "rdbu = px.colors.diverging.RdBu  # red-blue diverging\n",
    "\n",
    "colorscale = [\n",
    "    [0.0, grey],\n",
    "    [eps, grey],\n",
    "]\n",
    "for i, c in enumerate(rdbu):\n",
    "    t = eps + (1 - eps) * (i / (len(rdbu) - 1))\n",
    "    colorscale.append([t, c])\n",
    "\n",
    "# ---- geojson ----\n",
    "brazil_states = requests.get(\n",
    "    \"https://raw.githubusercontent.com/codeforamerica/click_that_hood/master/public/data/brazil-states.geojson\"\n",
    ").json()\n",
    "\n",
    "# ---- map ----\n",
    "fig = px.choropleth(\n",
    "    plot_df,\n",
    "    geojson=brazil_states,\n",
    "    locations=\"uf\",\n",
    "    featureidkey=\"properties.sigla\",\n",
    "    color=\"fill\",\n",
    "    color_continuous_scale=colorscale,\n",
    "    range_color=(sentinel, vmax),\n",
    "    hover_name=\"uf\",\n",
    "    hover_data={\n",
    "        \"mean_soy\": \":,.0f\",\n",
    "        \"mean_climate_change_pp\": \":.2f\",\n",
    "        \"adjusted_imports\": \":,.0f\",\n",
    "        \"import_change\": \":,.0f\",\n",
    "        \"uf\": True,\n",
    "        \"fill\": False,\n",
    "    },\n",
    "    title=\"NL Soy Imports from Brazil ‚Äî Climate-adjusted Import Change by State\",\n",
    ")\n",
    "\n",
    "fig.update_geos(\n",
    "    visible=False,\n",
    "    scope=\"south america\",\n",
    "    projection_type=\"mercator\",\n",
    "    lonaxis_range=[-75, -30],\n",
    "    lataxis_range=[-35, 6],\n",
    ")\n",
    "\n",
    "fig.update_traces(marker_line_color=\"black\", marker_line_width=0.6)\n",
    "\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=50, b=0),\n",
    "    coloraxis_colorbar=dict(title=\"Import change (tonnes)\"),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
